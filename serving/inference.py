"""
MindSporeç³–å°¿ç—…é¢„æµ‹æ¨ç†é€»è¾‘
"""
import os
import time
import json
import psutil
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
import mindspore as ms
from mindspore import Tensor, dtype as mstype

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.model_config import ModelConfig
from data.data_processor import DiabetesDataProcessor
from model.model_utils import load_model, predict_single_sample

class DiabetesPredictor:
    """ç³–å°¿ç—…é¢„æµ‹å™¨"""
    
    def __init__(self, model_path: str, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.model_path = model_path
        self.config_path = config_path
        self.model = None
        self.config = None
        self.data_processor = None
        self.is_loaded = False
        
        # æ€§èƒ½ç»Ÿè®¡
        self.prediction_count = 0
        self.total_inference_time = 0.0
        
        # åŠ è½½æ¨¡å‹å’Œé…ç½®
        self._load_model_and_config()
        
    def _load_model_and_config(self):
        """åŠ è½½æ¨¡å‹å’Œé…ç½®"""
        try:
            print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {self.model_path}")
            
            # åŠ è½½é…ç½®
            if self.config_path and os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config_dict = json.load(f)
                self.config = ModelConfig(**config_dict)
            else:
                # å°è¯•è‡ªåŠ¨æŸ¥æ‰¾é…ç½®æ–‡ä»¶
                auto_config_path = self.model_path.replace('.ckpt', '_config.json')
                if os.path.exists(auto_config_path):
                    with open(auto_config_path, 'r', encoding='utf-8') as f:
                        config_dict = json.load(f)
                    self.config = ModelConfig(**config_dict)
                else:
                    self.config = ModelConfig()
            
            # åŠ è½½æ¨¡å‹
            self.model = load_model(self.model_path, self.config)
            
            # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
            self.data_processor = DiabetesDataProcessor(model_config=self.config)
            
            self.is_loaded = True
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def predict_single(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å•æ ·æœ¬é¢„æµ‹
        
        Args:
            input_data: è¾“å…¥æ•°æ®å­—å…¸
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        start_time = time.time()
        
        try:
            # é¢„å¤„ç†è¾“å…¥æ•°æ®
            features = self.data_processor.preprocess_single_sample(input_data)
            
            # è¿›è¡Œé¢„æµ‹
            prediction_label, probability = predict_single_sample(self.model, features)
            
            # è®¡ç®—ç½®ä¿¡åº¦å’Œé£é™©ç­‰çº§
            confidence = self._calculate_confidence(probability)
            risk_level = self._get_risk_level(probability)
            
            # æ›´æ–°ç»Ÿè®¡
            inference_time = time.time() - start_time
            self.prediction_count += 1
            self.total_inference_time += inference_time
            
            return {
                "prediction": int(prediction_label),
                "probability": float(probability),
                "confidence": confidence,
                "risk_level": risk_level,
                "prediction_text": "æœ‰ç³–å°¿ç—…" if prediction_label == 1 else "æ— ç³–å°¿ç—…",
                "inference_time_ms": round(inference_time * 1000, 2)
            }
            
        except Exception as e:
            raise RuntimeError(f"é¢„æµ‹å¤±è´¥: {str(e)}")
    
    def predict_batch(self, input_data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡é¢„æµ‹
        
        Args:
            input_data_list: è¾“å…¥æ•°æ®åˆ—è¡¨
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        start_time = time.time()
        
        try:
            # é¢„å¤„ç†æ‰€æœ‰è¾“å…¥æ•°æ®
            features_list = []
            for input_data in input_data_list:
                features = self.data_processor.preprocess_single_sample(input_data)
                features_list.append(features.flatten())
            
            # æ‰¹é‡é¢„æµ‹
            batch_features = np.array(features_list)
            from model.model_utils import predict_batch
            prediction_labels, probabilities = predict_batch(self.model, batch_features)
            
            # æ„å»ºç»“æœ
            results = []
            for i, (label, prob) in enumerate(zip(prediction_labels, probabilities)):
                confidence = self._calculate_confidence(prob)
                risk_level = self._get_risk_level(prob)
                
                results.append({
                    "sample_id": i,
                    "prediction": int(label),
                    "probability": float(prob),
                    "confidence": confidence,
                    "risk_level": risk_level,
                    "prediction_text": "æœ‰ç³–å°¿ç—…" if label == 1 else "æ— ç³–å°¿ç—…"
                })
            
            # æ›´æ–°ç»Ÿè®¡
            inference_time = time.time() - start_time
            self.prediction_count += len(input_data_list)
            self.total_inference_time += inference_time
            
            return results
            
        except Exception as e:
            raise RuntimeError(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {str(e)}")
    
    def _calculate_confidence(self, probability: float) -> str:
        """è®¡ç®—ç½®ä¿¡åº¦ç­‰çº§"""
        if probability >= 0.9 or probability <= 0.1:
            return "é«˜"
        elif probability >= 0.75 or probability <= 0.25:
            return "ä¸­"
        else:
            return "ä½"
    
    def _get_risk_level(self, probability: float) -> str:
        """è·å–é£é™©ç­‰çº§"""
        if probability >= 0.8:
            return "é«˜é£é™©"
        elif probability >= 0.6:
            return "ä¸­é«˜é£é™©"
        elif probability >= 0.4:
            return "ä¸­ç­‰é£é™©"
        elif probability >= 0.2:
            return "ä½é£é™©"
        else:
            return "æä½é£é™©"
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if not self.is_loaded:
            return {"error": "æ¨¡å‹æœªåŠ è½½"}
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.size for p in self.model.trainable_params())
        
        info = {
            "model_path": self.model_path,
            "model_architecture": {
                "input_size": self.config.input_size,
                "hidden_sizes": self.config.hidden_sizes,
                "output_size": self.config.output_size,
                "dropout_rate": self.config.dropout_rate,
                "activation": self.config.activation
            },
            "parameters": {
                "total_trainable_params": int(total_params),
                "model_size_mb": os.path.getsize(self.model_path) / (1024 * 1024)
            },
            "features": {
                "feature_names": self.config.feature_names,
                "feature_count": len(self.config.feature_names)
            },
            "statistics": {
                "predictions_made": self.prediction_count,
                "avg_inference_time_ms": round(
                    (self.total_inference_time / self.prediction_count * 1000) 
                    if self.prediction_count > 0 else 0, 2
                )
            },
            "status": "loaded" if self.is_loaded else "not_loaded"
        }
        
        return info
    
    def get_feature_importance(self) -> Dict[str, Any]:
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        feature_importance = {
            "HbA1c_level": 0.25,
            "blood_glucose_level": 0.22,
            "bmi": 0.18,
            "age": 0.15,
            "smoking_history_encoded": 0.08,
            "hypertension": 0.06,
            "heart_disease": 0.04,
            "gender_encoded": 0.02,
            "age_group_encoded": 0.01
        }
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_features = sorted(feature_importance.items(), 
                               key=lambda x: x[1], reverse=True)
        
        return {
            "feature_importance": dict(sorted_features),
            "top_features": [f[0] for f in sorted_features[:5]],
            "note": "è¿™æ˜¯åŸºäºé¢†åŸŸçŸ¥è¯†çš„è¿‘ä¼¼é‡è¦æ€§"
        }
    
    def benchmark(self, num_samples: int = 100) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        print(f"ğŸƒ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• ({num_samples} æ ·æœ¬)...")
        
        # ç”Ÿæˆéšæœºæµ‹è¯•æ•°æ®
        test_data = []
        for _ in range(num_samples):
            sample = {
                "age": np.random.randint(20, 80),
                "gender": np.random.choice(["Male", "Female"]),
                "bmi": np.random.uniform(18, 40),
                "HbA1c_level": np.random.uniform(4, 10),
                "blood_glucose_level": np.random.uniform(80, 300),
                "smoking_history": np.random.choice(["never", "former", "current"]),
                "hypertension": np.random.choice([0, 1]),
                "heart_disease": np.random.choice([0, 1])
            }
            test_data.append(sample)
        
        # å•æ ·æœ¬åŸºå‡†æµ‹è¯•
        single_times = []
        for sample in test_data[:min(50, num_samples)]:
            start_time = time.time()
            _ = self.predict_single(sample)
            single_times.append((time.time() - start_time) * 1000)
        
        # æ‰¹é‡åŸºå‡†æµ‹è¯•
        start_time = time.time()
        _ = self.predict_batch(test_data)
        batch_time = (time.time() - start_time) * 1000
        
        return {
            "single_prediction": {
                "avg_time_ms": round(np.mean(single_times), 2),
                "min_time_ms": round(np.min(single_times), 2),
                "max_time_ms": round(np.max(single_times), 2),
                "std_time_ms": round(np.std(single_times), 2),
                "samples_tested": len(single_times)
            },
            "batch_prediction": {
                "total_time_ms": round(batch_time, 2),
                "avg_time_per_sample_ms": round(batch_time / num_samples, 2),
                "throughput_samples_per_sec": round(num_samples / (batch_time / 1000), 2),
                "batch_size": num_samples
            }
        }
    
    def get_memory_usage(self) -> Dict[str, float]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return {
            "rss_mb": round(memory_info.rss / (1024 * 1024), 2),
            "vms_mb": round(memory_info.vms / (1024 * 1024), 2),
            "percent": round(process.memory_percent(), 2)
        }

def create_sample_input() -> Dict[str, Any]:
    """åˆ›å»ºç¤ºä¾‹è¾“å…¥æ•°æ®"""
    return {
        "age": 45,
        "gender": "Male",
        "bmi": 28.5,
        "HbA1c_level": 6.5,
        "blood_glucose_level": 140,
        "smoking_history": "former",
        "hypertension": 1,
        "heart_disease": 0
    }

if __name__ == "__main__":
    # æµ‹è¯•æ¨ç†å™¨
    print("ğŸ§ª æµ‹è¯•MindSporeæ¨ç†å™¨...")
    
    # è®¾ç½®MindSporeä¸Šä¸‹æ–‡
    ms.set_context(mode=ms.GRAPH_MODE, device_target="CPU")
    
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å®é™…çš„æ¨¡å‹æ–‡ä»¶
    model_path = "./checkpoints/diabetes_model.ckpt"
    
    if os.path.exists(model_path):
        try:
            predictor = DiabetesPredictor(model_path)
            
            # æµ‹è¯•å•æ ·æœ¬é¢„æµ‹
            sample = create_sample_input()
            result = predictor.predict_single(sample)
            print("å•æ ·æœ¬é¢„æµ‹ç»“æœ:", result)
            
            # æµ‹è¯•æ‰¹é‡é¢„æµ‹
            batch_samples = [sample, sample, sample]
            batch_results = predictor.predict_batch(batch_samples)
            print("æ‰¹é‡é¢„æµ‹ç»“æœ:", len(batch_results))
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            model_info = predictor.get_model_info()
            print("æ¨¡å‹ä¿¡æ¯:", model_info)
            
            print("âœ… æ¨ç†å™¨æµ‹è¯•å®Œæˆ!")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
    else:
        print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹") 