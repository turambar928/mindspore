"""
MindSporeç³–å°¿ç—…é¢„æµ‹APIæœåŠ¡
"""
import os
import json
import time
from typing import Dict, List, Any, Union
from datetime import datetime
import numpy as np

from flask import Flask, request, jsonify
from flask_cors import CORS
import mindspore as ms
from mindspore import Tensor, dtype as mstype

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.model_config import ModelConfig, ServingConfig
from data.data_processor import DiabetesDataProcessor
from model.model_utils import load_model
from serving.inference import DiabetesPredictor

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€å˜é‡
predictor = None
serving_config = ServingConfig()
request_count = 0
start_time = time.time()

def initialize_service():
    """åˆå§‹åŒ–æœåŠ¡"""
    global predictor
    
    print("ğŸš€ åˆå§‹åŒ–MindSporeç³–å°¿ç—…é¢„æµ‹æœåŠ¡...")
    
    # è®¾ç½®MindSporeä¸Šä¸‹æ–‡
    ms.set_context(mode=ms.GRAPH_MODE, device_target="CPU")
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    try:
        predictor = DiabetesPredictor(
            model_path=serving_config.model_path,
            config_path=None
        )
        print("âœ… æœåŠ¡åˆå§‹åŒ–æˆåŠŸ!")
        return True
    except Exception as e:
        print(f"âŒ æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return False

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    global request_count, start_time
    
    request_count += 1
    uptime = time.time() - start_time
    
    return jsonify({
        "status": "healthy",
        "service": "mindspore-diabetes-prediction",
        "version": "1.0.0",
        "uptime_seconds": round(uptime, 2),
        "requests_served": request_count,
        "model_loaded": predictor is not None,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/predict', methods=['POST'])
def predict_single():
    """å•æ ·æœ¬é¢„æµ‹æ¥å£"""
    global request_count
    request_count += 1
    
    try:
        # éªŒè¯è¯·æ±‚
        if not request.json:
            return jsonify({"error": "è¯·æ±‚ä½“å¿…é¡»æ˜¯JSONæ ¼å¼"}), 400
        
        data = request.json
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['age', 'gender', 'bmi', 'HbA1c_level', 
                          'blood_glucose_level', 'smoking_history', 
                          'hypertension', 'heart_disease']
        
        missing_fields = [field for field in required_fields if field not in data]
        if missing_fields:
            return jsonify({
                "error": f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}"
            }), 400
        
        # è¿›è¡Œé¢„æµ‹
        start_time = time.time()
        result = predictor.predict_single(data)
        inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        response = {
            "prediction": result["prediction"],
            "probability": result["probability"],
            "risk_level": result["risk_level"],
            "confidence": result["confidence"],
            "inference_time_ms": round(inference_time, 2),
            "input_data": data,
            "timestamp": datetime.now().isoformat()
        }
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({
            "error": "é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
            "details": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/predict_batch', methods=['POST'])
def predict_batch():
    """æ‰¹é‡é¢„æµ‹æ¥å£"""
    global request_count
    request_count += 1
    
    try:
        # éªŒè¯è¯·æ±‚
        if not request.json:
            return jsonify({"error": "è¯·æ±‚ä½“å¿…é¡»æ˜¯JSONæ ¼å¼"}), 400
        
        data = request.json
        
        if 'samples' not in data:
            return jsonify({"error": "è¯·æ±‚å¿…é¡»åŒ…å«'samples'å­—æ®µ"}), 400
        
        samples = data['samples']
        if not isinstance(samples, list):
            return jsonify({"error": "'samples'å¿…é¡»æ˜¯åˆ—è¡¨"}), 400
        
        if len(samples) > serving_config.max_batch_size:
            return jsonify({
                "error": f"æ‰¹é‡å¤§å°è¶…è¿‡é™åˆ¶ (æœ€å¤§: {serving_config.max_batch_size})"
            }), 400
        
        # è¿›è¡Œæ‰¹é‡é¢„æµ‹
        start_time = time.time()
        results = predictor.predict_batch(samples)
        inference_time = (time.time() - start_time) * 1000
        
        response = {
            "predictions": results,
            "batch_size": len(samples),
            "inference_time_ms": round(inference_time, 2),
            "avg_time_per_sample_ms": round(inference_time / len(samples), 2),
            "timestamp": datetime.now().isoformat()
        }
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({
            "error": "æ‰¹é‡é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
            "details": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/model_info', methods=['GET'])
def model_info():
    """æ¨¡å‹ä¿¡æ¯æ¥å£"""
    try:
        info = predictor.get_model_info()
        return jsonify(info)
    except Exception as e:
        return jsonify({
            "error": "è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥",
            "details": str(e)
        }), 500

@app.route('/feature_importance', methods=['GET'])
def feature_importance():
    """ç‰¹å¾é‡è¦æ€§æ¥å£"""
    try:
        importance = predictor.get_feature_importance()
        return jsonify(importance)
    except Exception as e:
        return jsonify({
            "error": "è·å–ç‰¹å¾é‡è¦æ€§å¤±è´¥",
            "details": str(e)
        }), 500

@app.route('/benchmark', methods=['POST'])
def benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•æ¥å£"""
    try:
        data = request.json
        num_samples = data.get('num_samples', 100)
        
        if num_samples > 1000:
            return jsonify({"error": "åŸºå‡†æµ‹è¯•æ ·æœ¬æ•°é‡ä¸èƒ½è¶…è¿‡1000"}), 400
        
        result = predictor.benchmark(num_samples)
        return jsonify(result)
        
    except Exception as e:
        return jsonify({
            "error": "æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥",
            "details": str(e)
        }), 500

@app.route('/stats', methods=['GET'])
def service_stats():
    """æœåŠ¡ç»Ÿè®¡æ¥å£"""
    global request_count, start_time
    
    uptime = time.time() - start_time
    
    stats = {
        "service_uptime_seconds": round(uptime, 2),
        "total_requests": request_count,
        "requests_per_minute": round(request_count / (uptime / 60), 2) if uptime > 0 else 0,
        "memory_usage": predictor.get_memory_usage() if predictor else "N/A",
        "model_loaded": predictor is not None,
        "timestamp": datetime.now().isoformat()
    }
    
    return jsonify(stats)

@app.errorhandler(404)
def not_found(error):
    """404é”™è¯¯å¤„ç†"""
    return jsonify({
        "error": "æ¥å£ä¸å­˜åœ¨",
        "available_endpoints": [
            "/health",
            "/predict",
            "/predict_batch", 
            "/model_info",
            "/feature_importance",
            "/benchmark",
            "/stats"
        ]
    }), 404

@app.errorhandler(500)
def internal_error(error):
    """500é”™è¯¯å¤„ç†"""
    return jsonify({
        "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
        "details": str(error)
    }), 500

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  MindSporeç³–å°¿ç—…é¢„æµ‹APIæœåŠ¡")
    print("=" * 50)
    
    # åˆå§‹åŒ–æœåŠ¡
    if not initialize_service():
        print("âŒ æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡º")
        return 1
    
    # å¯åŠ¨æœåŠ¡
    print(f"ğŸŒ å¯åŠ¨APIæœåŠ¡...")
    print(f"åœ°å€: http://{serving_config.host}:{serving_config.port}")
    print(f"å·¥ä½œè¿›ç¨‹: {serving_config.workers}")
    print("=" * 50)
    
    app.run(
        host=serving_config.host,
        port=serving_config.port,
        debug=False,
        threaded=True
    )

if __name__ == "__main__":
    main() 