"""
MindSporeç³–å°¿ç—…é¢„æµ‹æ¨¡å‹è¯„ä¼°è„šæœ¬
"""
import os
import argparse
import mindspore as ms
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc
import json

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.model_config import ModelConfig
from data.data_processor import prepare_data_for_training, DiabetesDataProcessor
from model.model_utils import load_model, evaluate_model

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='MindSporeç³–å°¿ç—…é¢„æµ‹æ¨¡å‹è¯„ä¼°')
    
    parser.add_argument('--model_path', type=str, required=True,
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_path', type=str, 
                       default="../diabetes_prediction_dataset.csv",
                       help='æµ‹è¯•æ•°æ®è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./evaluation_results',
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹å¤§å°')
    parser.add_argument('--device', type=str, default='CPU',
                       choices=['CPU', 'GPU', 'Ascend'],
                       help='è®¡ç®—è®¾å¤‡')
    
    return parser.parse_args()

def detailed_evaluation(model, test_dataset, output_dir):
    """è¯¦ç»†è¯„ä¼°æ¨¡å‹"""
    print("ğŸ” è¿›è¡Œè¯¦ç»†æ¨¡å‹è¯„ä¼°...")
    
    model.set_train(False)
    
    all_predictions = []
    all_probabilities = []
    all_targets = []
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ
    for batch in test_dataset.create_dict_iterator():
        features = batch['features']
        labels = batch['label']
        
        predictions = model(features)
        probabilities = predictions.asnumpy().flatten()
        targets = labels.asnumpy().flatten()
        
        all_probabilities.extend(probabilities)
        all_targets.extend(targets)
        all_predictions.extend((probabilities > 0.5).astype(int))
    
    all_predictions = np.array(all_predictions)
    all_probabilities = np.array(all_probabilities)
    all_targets = np.array(all_targets)
    
    # åŸºæœ¬æŒ‡æ ‡
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    metrics = {
        'accuracy': accuracy_score(all_targets, all_predictions),
        'precision': precision_score(all_targets, all_predictions),
        'recall': recall_score(all_targets, all_predictions),
        'f1': f1_score(all_targets, all_predictions)
    }
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_targets, all_predictions)
    
    # ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(all_targets, all_probabilities)
    roc_auc = auc(fpr, tpr)
    
    # ä¿å­˜ç»“æœ
    save_evaluation_results(metrics, cm, fpr, tpr, roc_auc, output_dir)
    
    # ç»˜åˆ¶å›¾è¡¨
    plot_evaluation_charts(metrics, cm, fpr, tpr, roc_auc, output_dir)
    
    return metrics, cm, roc_auc

def save_evaluation_results(metrics, cm, fpr, tpr, roc_auc, output_dir):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æŒ‡æ ‡
    results = {
        'metrics': metrics,
        'confusion_matrix': cm.tolist(),
        'roc_auc': float(roc_auc),
        'roc_curve': {
            'fpr': fpr.tolist(),
            'tpr': tpr.tolist()
        }
    }
    
    with open(os.path.join(output_dir, 'evaluation_results.json'), 'w') as f:
        json.dump(results, f, indent=2)
    
    # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
    with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w', encoding='utf-8') as f:
        f.write("MindSporeç³–å°¿ç—…é¢„æµ‹æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("æ€§èƒ½æŒ‡æ ‡:\n")
        for metric, value in metrics.items():
            f.write(f"  {metric}: {value:.4f}\n")
        
        f.write(f"\nROC AUC: {roc_auc:.4f}\n\n")
        
        f.write("æ··æ·†çŸ©é˜µ:\n")
        f.write(f"                é¢„æµ‹\n")
        f.write(f"å®é™…     0    1\n")
        f.write(f"  0   {cm[0,0]:4d} {cm[0,1]:4d}\n")
        f.write(f"  1   {cm[1,0]:4d} {cm[1,1]:4d}\n")

def plot_evaluation_charts(metrics, cm, fpr, tpr, roc_auc, output_dir):
    """ç»˜åˆ¶è¯„ä¼°å›¾è¡¨"""
    plt.style.use('default')
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # æŒ‡æ ‡æ¡å½¢å›¾
    ax1 = axes[0, 0]
    metric_names = list(metrics.keys())
    metric_values = list(metrics.values())
    bars = ax1.bar(metric_names, metric_values, color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D'])
    ax1.set_title('æ¨¡å‹æ€§èƒ½æŒ‡æ ‡')
    ax1.set_ylabel('åˆ†æ•°')
    ax1.set_ylim(0, 1)
    
    # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼
    for bar, value in zip(bars, metric_values):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
    ax2 = axes[0, 1]
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,
                xticklabels=['é¢„æµ‹ 0', 'é¢„æµ‹ 1'],
                yticklabels=['å®é™… 0', 'å®é™… 1'])
    ax2.set_title('æ··æ·†çŸ©é˜µ')
    
    # ROCæ›²çº¿
    ax3 = axes[1, 0]
    ax3.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROC curve (AUC = {roc_auc:.3f})')
    ax3.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    ax3.set_xlim([0.0, 1.0])
    ax3.set_ylim([0.0, 1.05])
    ax3.set_xlabel('å‡é˜³æ€§ç‡')
    ax3.set_ylabel('çœŸé˜³æ€§ç‡')
    ax3.set_title('ROCæ›²çº¿')
    ax3.legend(loc="lower right")
    ax3.grid(True, alpha=0.3)
    
    # æ¦‚ç‡åˆ†å¸ƒ
    ax4 = axes[1, 1]
    ax4.text(0.1, 0.5, f'æ¨¡å‹è¯„ä¼°å®Œæˆ\n\nå‡†ç¡®ç‡: {metrics["accuracy"]:.3f}\n'
                       f'ç²¾ç¡®ç‡: {metrics["precision"]:.3f}\n'
                       f'å¬å›ç‡: {metrics["recall"]:.3f}\n'
                       f'F1åˆ†æ•°: {metrics["f1"]:.3f}\n'
                       f'ROC AUC: {roc_auc:.3f}',
             transform=ax4.transAxes, fontsize=12,
             verticalalignment='center',
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    ax4.set_title('è¯„ä¼°æ‘˜è¦')
    ax4.axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'evaluation_charts.png'), dpi=300, bbox_inches='tight')
    plt.show()

def compare_with_baseline(metrics, output_dir):
    """ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ"""
    # ç®€å•åŸºçº¿ï¼šå§‹ç»ˆé¢„æµ‹å¤šæ•°ç±»
    baseline_accuracy = 0.913  # æ¥è‡ªæ•°æ®åˆ†æ
    
    improvement = {
        'accuracy_improvement': metrics['accuracy'] - baseline_accuracy,
        'precision': metrics['precision'],
        'recall': metrics['recall'],
        'f1': metrics['f1']
    }
    
    print(f"\nğŸ“Š ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ:")
    print(f"åŸºçº¿å‡†ç¡®ç‡ (å¤šæ•°ç±»): {baseline_accuracy:.3f}")
    print(f"æ¨¡å‹å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
    print(f"å‡†ç¡®ç‡æå‡: {improvement['accuracy_improvement']:.3f}")
    
    return improvement

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # è®¾ç½®ç¯å¢ƒ
    ms.set_context(mode=ms.GRAPH_MODE, device_target=args.device)
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(args.model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return 1
    
    if not os.path.exists(args.data_path):
        print(f"âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_path}")
        return 1
    
    print("ğŸ§  MindSporeç³–å°¿ç—…é¢„æµ‹æ¨¡å‹è¯„ä¼°")
    print("=" * 50)
    
    try:
        # åŠ è½½æ¨¡å‹
        print("ğŸ“‚ åŠ è½½æ¨¡å‹...")
        model = load_model(args.model_path)
        
        # å‡†å¤‡æ•°æ®
        print("ğŸ“Š å‡†å¤‡æµ‹è¯•æ•°æ®...")
        _, _, test_dataset = prepare_data_for_training(
            data_path=args.data_path,
            batch_size=args.batch_size
        )
        
        print(f"æµ‹è¯•é›†å¤§å°: {test_dataset.get_dataset_size()}")
        
        # åŸºæœ¬è¯„ä¼°
        print("\nğŸ” åŸºæœ¬æ¨¡å‹è¯„ä¼°...")
        basic_metrics = evaluate_model(model, test_dataset)
        
        print("åŸºæœ¬æŒ‡æ ‡:")
        for metric, value in basic_metrics.items():
            print(f"  {metric}: {value:.4f}")
        
        # è¯¦ç»†è¯„ä¼°
        print("\nğŸ”¬ è¯¦ç»†æ¨¡å‹è¯„ä¼°...")
        detailed_metrics, cm, roc_auc = detailed_evaluation(model, test_dataset, args.output_dir)
        
        # ä¸åŸºçº¿æ¯”è¾ƒ
        improvement = compare_with_baseline(detailed_metrics, args.output_dir)
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
        print(f"æœ€ç»ˆå‡†ç¡®ç‡: {detailed_metrics['accuracy']:.4f}")
        print(f"ROC AUC: {roc_auc:.4f}")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 